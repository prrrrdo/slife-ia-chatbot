# Arquivo: versao_2_estruturada/app/services/rag_service.py
import os
import pandas as pd
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from app.config import settings

# Armazenamento de histÃ³rico em memÃ³ria RAM
store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

class RAGService:
    def __init__(self):
        print("ðŸš€ Inicializando ServiÃ§o RAG...")
        
        # VERIFICAÃ‡ÃƒO DE SEGURANÃ‡A
        if not settings.GOOGLE_API_KEY:
            print("âŒ ERRO CRÃTICO: A chave GOOGLE_API_KEY nÃ£o foi encontrada.")
            print("   Verifique se o arquivo .env estÃ¡ na raiz do projeto 'atividade-ia-slife'")
            raise ValueError("Chave de API ausente")

        # 1. Carregar e Processar Dados do CSV
        docs = self._carregar_dados_csv()
        
        if not docs:
            raise RuntimeError("âŒ Falha crÃ­tica: Nenhum dado foi carregado do CSV.")

        # 2. Criar Banco Vetorial em MemÃ³ria (FAISS)
        print("ðŸ§  Criando Ã­ndice vetorial na memÃ³ria...")
        try:
            # AQUI ESTÃ A CORREÃ‡ÃƒO PRINCIPAL: google_api_key=...
            embeddings = GoogleGenerativeAIEmbeddings(
                model="models/text-embedding-004",
                google_api_key=settings.GOOGLE_API_KEY 
            )
            
            vector_store = FAISS.from_documents(docs, embeddings)
            
            retriever = vector_store.as_retriever(
                search_type="mmr", 
                search_kwargs={"k": 20, "fetch_k": 100, "lambda_mult": 0.6}
            )
            print("âœ… Banco vetorial criado com sucesso!")
        except Exception as e:
            raise RuntimeError(f"Erro ao criar vetores (Embeddings): {e}")

        # 3. Configurar o Modelo Gemini
        # AQUI TAMBÃ‰M: google_api_key=...
        llm = ChatGoogleGenerativeAI(
            model=settings.MODEL_NAME, 
            temperature=0.7,
            google_api_key=settings.GOOGLE_API_KEY
        )

        # 4. Configurar as Chains
        
        # A) ReformulaÃ§Ã£o
        contextualize_q_system_prompt = """
        Dado um histÃ³rico de chat e a Ãºltima pergunta do usuÃ¡rio, 
        formule uma pergunta autÃ´noma que possa ser entendida sem o histÃ³rico. 
        NÃƒO responda, apenas reformule se necessÃ¡rio.
        """
        contextualize_q_prompt = ChatPromptTemplate.from_messages([
            ("system", contextualize_q_system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])
        
        history_aware_retriever = (
            contextualize_q_prompt
            | llm
            | StrOutputParser()
            | retriever
        )

        # B) Resposta Final
        qa_system_prompt = """
        VocÃª Ã© o assistente virtual da SLife (Moradia UniversitÃ¡ria).
        
        DIRETRIZES:
        1. Use os contextos fornecidos para responder.
        2. Se encontrar imÃ³veis, cite o ID, Cidade e Valor.
        3. Seja cordial e objetivo.
        
        CONTEXTO:
        {context}
        """
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", qa_system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])

        question_answer_chain = (
            RunnablePassthrough.assign(context=history_aware_retriever)
            | qa_prompt
            | llm
            | StrOutputParser()
        )

        self.chain = RunnableWithMessageHistory(
            question_answer_chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
        )

    def _carregar_dados_csv(self):
        print(f"ðŸ“‚ Lendo CSV: {settings.CSV_PATH}")
        if not os.path.exists(settings.CSV_PATH):
            print(f"âŒ Arquivo nÃ£o encontrado: {settings.CSV_PATH}")
            return []

        try:
            df = pd.read_csv(settings.CSV_PATH, sep=';', decimal=',')
            documentos = []
            for _, row in df.iterrows():
                texto = (
                    f"ImÃ³vel ID {row['imovel_id']} tipo {row['tipo']} em {row['cidade']}. "
                    f"Valor: R$ {row['valor_aluguel']}. "
                    f"{row['quartos']} quartos. MobÃ­lia: {'Sim' if row['tem_mobilia'] else 'NÃ£o'}. "
                )
                documentos.append(Document(page_content=texto, metadata={"id": row['imovel_id']}))
            
            print(f"ðŸ“Š {len(documentos)} imÃ³veis carregados.")
            return documentos
        except Exception as e:
            print(f"âŒ Erro CSV: {e}")
            return []

    def get_response(self, session_id: str, message: str) -> str:
        return self.chain.invoke(
            {"input": message},
            config={"configurable": {"session_id": session_id}}
        )

rag_service = RAGService()