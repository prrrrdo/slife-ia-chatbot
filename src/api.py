import os
import sys

# Adiciona o diret√≥rio atual ao path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel 
from dotenv import load_dotenv

# --- NOVAS IMPORTA√á√ïES PARA MEM√ìRIA ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Importa fun√ß√µes do motor de busca
try:
    from app_ia import carregar_dados, criar_indice_vetorial
except ImportError:
    from src.app_ia import carregar_dados, criar_indice_vetorial

load_dotenv()

app = FastAPI(title="API SLife - Chatbot com Mem√≥ria", version="1.2.0")

# --- BLOCO DE SEGURAN√áA (CORS) ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Gest√£o de Sess√£o (Mem√≥ria RAM) ---
# Aqui guardamos o hist√≥rico de cada usu√°rio enquanto o servidor roda
store = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

# --- Vari√°veis Globais ---
rag_chain_with_history = None

class UserRequest(BaseModel):
    message: str
    session_id: str = "usuario_padrao" # Identificador da conversa

@app.on_event("startup")
async def startup_event():
    global rag_chain_with_history
    print("üöÄ Inicializando API com Mem√≥ria...")

    caminho_csv = "data/slife_imoveis.csv"
    if not os.path.exists(caminho_csv):
        caminho_csv = "../data/slife_imoveis.csv"
    
    if not os.path.exists(caminho_csv):
        print("‚ùå ERRO: CSV n√£o encontrado.")
        return

    # 1. Carregar Dados
    print("üìÇ Carregando dados...")
    docs = carregar_dados(caminho_csv)
    if not docs:
        print("‚ùå Falha ao carregar documentos.")
        return
        
    vector_store = criar_indice_vetorial(docs)
    
    # 2. Retriever com MMR (Diversidade)
    # k=20 com MMR √© suficiente se a busca for bem feita
    retriever = vector_store.as_retriever(
        search_type="mmr", 
        search_kwargs={"k": 20, "fetch_k": 100, "lambda_mult": 0.6}
    )

    if not os.getenv("GOOGLE_API_KEY"):
        print("‚ùå ERRO: Sem API KEY.")
        return
        
    # Modelo Gemini (Usando a vers√£o flash est√°vel)
    chat_model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)

    # --- C√âREBRO DA MEM√ìRIA ---
    
    # PASSO A: Reformulador de Perguntas
    # Se o usu√°rio diz "E com lavanderia?", a IA transforma em "Im√≥veis em [cidade anterior] com lavanderia"
    contextualize_q_system_prompt = """
    Dado um hist√≥rico de chat e a √∫ltima pergunta do usu√°rio 
    (que pode fazer refer√™ncia ao contexto passado), formule uma pergunta aut√¥noma 
    que possa ser entendida sem o hist√≥rico. N√ÉO responda √† pergunta, 
    apenas a reformule se necess√°rio ou retorne-a como est√°.
    """
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ])
    
    history_aware_retriever = (
        contextualize_q_prompt
        | chat_model
        | StrOutputParser()
        | retriever
    )

    # PASSO B: Resposta Final (QA)
    qa_system_prompt = """
    Voc√™ √© o assistente da SLife (Moradia Universit√°ria). Jovem, √∫til e direto.
    
    DIRETRIZES:
    1. Use os contextos abaixo para responder.
    2. Se o usu√°rio pediu perfil (sil√™ncio vs festa), filtre mentalmente.
    3. Cite ID, Cidade e Valor das op√ß√µes.
    
    CONTEXTO:
    {context}
    
    Responda em portugu√™s do Brasil.
    """
    
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        ("placeholder", "{chat_history}"), # Hist√≥rico entra aqui
        ("human", "{input}"),
    ])

    question_answer_chain = (
        RunnablePassthrough.assign(context=history_aware_retriever)
        | qa_prompt
        | chat_model
        | StrOutputParser()
    )

    # PASSO C: Chain Final com Gest√£o de Hist√≥rico
    rag_chain_with_history = RunnableWithMessageHistory(
        question_answer_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
    )
    
    print("‚úÖ API com Mem√≥ria Pronta!")

@app.post("/chat")
async def chat_endpoint(request: UserRequest):
    if not rag_chain_with_history:
        raise HTTPException(status_code=500, detail="IA off.")
    
    try:
        print(f"üì© Msg: {request.message} | Session: {request.session_id}")
        
        # Invocamos passando o session_id para recuperar o hist√≥rico correto
        resposta = rag_chain_with_history.invoke(
            {"input": request.message},
            config={"configurable": {"session_id": request.session_id}}
        )
        return {"response": resposta}
    except Exception as e:
        print(f"Erro: {e}")
        raise HTTPException(status_code=500, detail=str(e))