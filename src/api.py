# api.py

import os # importa ferramentas do SO para ler pastas, caminhos, variaveis de ambiente , etc.
import sys # Importa ferramentas do interpretador python

# Garante que o Python encontre os m√≥dulos no diret√≥rio atual (evita erros de importa√ß√£o)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fastapi import FastAPI, HTTPException # #classe principal, httoexception para retornar esseos de chamada da api
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel 
from dotenv import load_dotenv

# --- LANGCHAIN ---
from langchain_google_genai import ChatGoogleGenerativeAI # O modelo de chat (Gemini Flash)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # Para criar templates de conversa
from langchain_core.runnables import RunnablePassthrough # Conecta etapas do pipeline
from langchain_core.output_parsers import StrOutputParser # Limpa a sa√≠da para ser s√≥ texto (string)
from langchain_core.chat_history import InMemoryChatMessageHistory # Mem√≥ria RAM
from langchain_core.runnables.history import RunnableWithMessageHistory # Gerenciador de hist√≥rico

# Importa as fun√ß√µes que explicamos no arquivo anterior
try:
    from app_ia import carregar_dados, criar_indice_vetorial
except ImportError:
    from src.app_ia import carregar_dados, criar_indice_vetorial

load_dotenv()

# Inicializa o servidor web
app = FastAPI(title="API SLife - Chatbot com Mem√≥ria", version="1.2.0")

# --- CORS (Cross-Origin Resource Sharing) ---
# Isso √© vital. Permite que um site (ex: localhost:3000 ou seu frontend) 
# fa√ßa requisi√ß√µes para este backend sem ser bloqueado pelo navegador.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Em produ√ß√£o, colocar apenas o dom√≠nio do site aqui.
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- GEST√ÉO DE MEM√ìRIA ---
# Um dicion√°rio simples para guardar as conversas.
# Chave: session_id (ex: "usuario_123"), Valor: Lista de mensagens.
# OBS: Se reiniciar o servidor, perde-se tudo (pois √© mem√≥ria RAM).
store = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    """Recupera ou cria o hist√≥rico de um usu√°rio espec√≠fico."""
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

# Vari√°vel global para guardar a "cadeia" de IA pronta para uso
rag_chain_with_history = None

# Modelo de dados que define o que o usu√°rio DEVE enviar no JSON (valida√ß√£o autom√°tica)
class UserRequest(BaseModel):
    message: str
    session_id: str = "usuario_padrao"

# --- INICIALIZA√á√ÉO (STARTUP) ---
# Executa apenas UMA VEZ quando o servidor liga.
@app.on_event("startup")
async def startup_event():
    global rag_chain_with_history
    print("üöÄ Inicializando API com Mem√≥ria...")

    # Tenta localizar o CSV (l√≥gica para funcionar localmente ou em pastas diferentes)
    caminho_csv = "data/slife_imoveis.csv"
    if not os.path.exists(caminho_csv):
        caminho_csv = "../data/slife_imoveis.csv"
    
    # 1. Pipeline de Dados (ETL + Vetoriza√ß√£o)
    # Chama as fun√ß√µes do app_ia.py para carregar e indexar os im√≥veis na mem√≥ria.
    docs = carregar_dados(caminho_csv)
    vector_store = criar_indice_vetorial(docs)
    
    # 2. Configura o Retriever (O "Buscador")
    # search_type="mmr" (Maximal Marginal Relevance): 
    # Em vez de pegar os 20 im√≥veis mais parecidos (que podem ser quase id√™nticos),
    # ele pega alguns parecidos e tenta variar um pouco entre eles para dar diversidade.
    retriever = vector_store.as_retriever(
        search_type="mmr", 
        search_kwargs={"k": 20, "fetch_k": 100, "lambda_mult": 0.6}
    )

    # Configura o Gemini (modelo Flash √© mais r√°pido e barato para chat)
    chat_model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)

    # --- RAG CHAIN: O C√âREBRO COMPLEXO ---
    
    # PASSO A: Contextualiza√ß√£o (Reformulador de Perguntas)
    # Problema: Se o usu√°rio diz "E qual o pre√ßo?", a busca vetorial falharia (pois n√£o sabe o sujeito).
    # Solu√ß√£o: A IA reescreve a pergunta baseada no hist√≥rico: "Qual o pre√ßo [do im√≥vel X que falamos antes]?"
    contextualize_q_system_prompt = """
    Dado um hist√≥rico de chat e a √∫ltima pergunta do usu√°rio, 
    formule uma pergunta aut√¥noma que possa ser entendida sem o hist√≥rico.
    """
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        ("placeholder", "{chat_history}"), # Aqui entra o hist√≥rico
        ("human", "{input}"), # Aqui entra a pergunta atual ("E o pre√ßo?")
    ])
    
    # Cria um mini-pipeline s√≥ para reformular a pergunta
    history_aware_retriever = (
        contextualize_q_prompt
        | chat_model
        | StrOutputParser()
        | retriever # Usa a pergunta reformulada para buscar no banco vetorial
    )

    # PASSO B: Resposta Final (QA - Question Answering)
    # Agora que temos os documentos (contexto), pedimos para a IA responder.
    qa_system_prompt = """
    Voc√™ √© o assistente da SLife (Moradia Universit√°ria). Jovem, √∫til e direto.
    Use os contextos abaixo para responder. Cite ID, Cidade e Valor.
    CONTEXTO: {context}
    """
    
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ])

    # Monta a cadeia principal:
    # 1. Pega a pergunta -> Reformula -> Busca documentos (history_aware_retriever)
    # 2. Joga os documentos + pergunta no Prompt (qa_prompt)
    # 3. Manda para o Gemini (chat_model)
    question_answer_chain = (
        RunnablePassthrough.assign(context=history_aware_retriever)
        | qa_prompt
        | chat_model
        | StrOutputParser()
    )

    # PASSO C: O Gerente de Sess√£o
    # Adiciona a capacidade autom√°tica de ler/gravar o hist√≥rico na vari√°vel 'store'
    rag_chain_with_history = RunnableWithMessageHistory(
        question_answer_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
    )
    
    print("‚úÖ API com Mem√≥ria Pronta!")

# --- ENDPOINT (A ROTA) ---
@app.post("/chat")
async def chat_endpoint(request: UserRequest):
    # Rota que o Frontend vai chamar.
    
    try:
        # Invoca a cadeia inteira.
        # O 'configurable' √© onde passamos o ID da sess√£o para o LangChain saber quem √© quem.
        resposta = rag_chain_with_history.invoke(
            {"input": request.message},
            config={"configurable": {"session_id": request.session_id}}
        )
        return {"response": resposta}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))